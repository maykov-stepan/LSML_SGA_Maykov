{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a websitt1. It allows you to understand how users interact with the sitt1. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_page (string)` - Page on the sitt1.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the sitt1.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leveragt1. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.t1.: no O^2 or O^3 complexities; appropriate object usage; no data leaks ett0. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code ett0. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import col, concat_ws, collect_list, expr, first, lit, min as spark_min\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Maykov\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\tsession_id\tevent_type\tevent_page\ttimestamp\n",
      "562\t507\tpage\tmain\t1695584127\n",
      "562\t507\tevent\tmain\t1695584134\n",
      "562\t507\tevent\tmain\t1695584144\n",
      "562\t507\tevent\tmain\t1695584147\n",
      "562\t507\twNaxLlerrorU\tmain\t1695584154\n",
      "562\t507\tevent\tmain\t1695584154\n",
      "562\t507\tevent\tmain\t1695584154\n",
      "562\t507\tevent\tmain\t1695584160\n",
      "562\t507\tpage\trabota\t1695584166\n",
      "562\t507\tevent\trabota\t1695584174\n",
      "562\t507\tevent\trabota\t1695584181\n",
      "562\t507\tevent\trabota\t1695584189\n",
      "562\t507\tpage\tmain\t1695584194\n",
      "562\t507\tevent\tmain\t1695584204\n",
      "562\t507\tevent\tmain\t1695584211\n",
      "562\t507\tevent\tmain\t1695584211\n",
      "562\t507\tevent\tmain\t1695584219\n",
      "562\t507\tpage\tbonus\t1695584221\n",
      "562\t507\tpage\tonline\t1695584222\n",
      "562\t507\tevent\tonline\t1695584230\n",
      "3539\t849\tpage\tmain\t1695584238\n",
      "3539\t849\tevent\tmain\t1695584252\n",
      "3539\t849\tpage\tonline\t1695584261\n",
      "3539\t849\tpage\tbonus\t1695584269\n",
      "3539\t849\tevent\tbonus\t1695584278\n",
      "3539\t849\tpage\tnews\t1695584285\n",
      "3539\t849\tpage\tmain\t1695584291\n",
      "3539\t849\tevent\tmain\t1695584301\n",
      "3539\t849\tpage\tnews\t1695584306\n",
      "3539\t849\tevent\tnews\t1695584307\n",
      "3539\t849\tpage\tvklad\t1695584317\n",
      "3539\t849\tpage\trabot"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -head /data/clickstream.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark SQL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_tab = f\"\"\"CREATE OR REPLACE TEMPORARY VIEW clnt_df USING csv OPTIONS ('path' '{\"hdfs:/data/clickstream.csv\"}','header' 'true','sep' '\\t')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_1 = \"\"\"\n",
    "SELECT distinct event_type\n",
    "FROM clnt_df\n",
    "where upper(event_type) not like '%ERROR%'\n",
    "limit 30;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|event_type|\n",
      "+----------+\n",
      "|page      |\n",
      "|event     |\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_tab)\n",
    "sql_result = spark.sql(sql_1)\n",
    "sql_result.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "except for the '%error%' like event types there are only 'page' and 'event' phases, so either \"in ('page', 'event')\" or \"not like '%error%'\" could be used, I will prefer the second option, as if the base will be ipdated with some new values the logic will not fail to catch every non error event. Moreover, I will use upper function to make the text of the same format and catch an '%ERROR%' even if it has a strange forms like '%eRror%'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_main_1 = \"\"\"\n",
    "with fin_table_0 as (\n",
    "select t0.user_id, t0.session_id, min(t0.timestamp) as min_error_time\n",
    "from clnt_df t0\n",
    "where upper(t0.event_type) like '%ERROR%'\n",
    "group by 1,2\n",
    ")\n",
    ", fin_table_1 as ( \n",
    "select t0.user_id, t0.session_id, t0.event_page, t0.timestamp\n",
    "from clnt_df t0\n",
    "left join fin_table_0 t1\n",
    "on t0.user_id = t1.user_id and t0.session_id = t1.session_id\n",
    "where upper(t0.event_type) not like '%ERROR%'\n",
    "and (t1.min_error_time is null or t0.timestamp < t1.min_error_time)\n",
    ")\n",
    ", fin_table_2 as (\n",
    "select distinct t0.user_id, t0.session_id, collect_list(t0.event_page) over (partition by t0.user_id, t0.session_id order by t0.timestamp, t0.event_page rows between unbounded preceding and unbounded following) as event_pages\n",
    "from fin_table_1 t0\n",
    "ORDER BY 1,2\n",
    ")\n",
    ", fin_table_3 as (\n",
    "select t0.user_id, t0.session_id, t0.event_pages,\n",
    "transform(t0.event_pages, (page, i) -> \n",
    "if(i = 0 or page != t0.event_pages[i - 1], page, null)\n",
    ") as unique_event_pages\n",
    "from fin_table_2 t0\n",
    ")\n",
    ", fin_table as (\n",
    "select distinct t0.user_id, t0.session_id, concat_ws('-', t0.unique_event_pages) as route\n",
    "from fin_table_3 t0\n",
    ")\n",
    "\n",
    "select route, count(session_id) as count\n",
    "from fin_table\n",
    "group by 1\n",
    "order by 2 desc\n",
    "limit 30;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|route                |count|\n",
      "+---------------------+-----+\n",
      "|main                 |8184 |\n",
      "|main-archive         |1112 |\n",
      "|main-rabota          |1047 |\n",
      "|main-internet        |895  |\n",
      "|main-bonus           |869  |\n",
      "|main-news            |769  |\n",
      "|main-tariffs         |677  |\n",
      "|main-online          |587  |\n",
      "|main-vklad           |518  |\n",
      "|main-rabota-archive  |170  |\n",
      "|main-archive-rabota  |167  |\n",
      "|main-bonus-archive   |143  |\n",
      "|main-rabota-bonus    |138  |\n",
      "|main-bonus-rabota    |135  |\n",
      "|main-news-rabota     |135  |\n",
      "|main-archive-internet|132  |\n",
      "|main-rabota-news     |130  |\n",
      "|main-internet-rabota |129  |\n",
      "|main-archive-news    |126  |\n",
      "|main-rabota-internet |124  |\n",
      "|main-internet-archive|123  |\n",
      "|main-archive-bonus   |117  |\n",
      "|main-internet-bonus  |115  |\n",
      "|main-tariffs-internet|114  |\n",
      "|main-news-archive    |113  |\n",
      "|main-news-internet   |108  |\n",
      "|main-archive-tariffs |104  |\n",
      "|main-internet-news   |103  |\n",
      "|main-tariffs-archive |103  |\n",
      "|main-rabota-main     |94   |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(sql_tab)\n",
    "spark_sql_result = spark.sql(sql_main_1)\n",
    "spark_sql_result.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_sql_result.toPandas().to_csv(\"spark_sql_result.csv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark RDD</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was previously mentioned except for the '%error%' like event types there are only 'page' and 'event' phases, so I have filtered such events \".upper() in ('PAGE', 'EVENT')\". Moreover, I will use upper function to make the text of the same format and catch an '%ERROR%' even if it has a strange forms like '%eRror%' like it was with spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('main', 8184),\n",
       " ('main-archive', 1112),\n",
       " ('main-rabota', 1047),\n",
       " ('main-internet', 895),\n",
       " ('main-bonus', 869),\n",
       " ('main-news', 769),\n",
       " ('main-tariffs', 677),\n",
       " ('main-online', 587),\n",
       " ('main-vklad', 518),\n",
       " ('main-rabota-archive', 170),\n",
       " ('main-archive-rabota', 167),\n",
       " ('main-bonus-archive', 143),\n",
       " ('main-rabota-bonus', 138),\n",
       " ('main-news-rabota', 135),\n",
       " ('main-bonus-rabota', 135),\n",
       " ('main-archive-internet', 132),\n",
       " ('main-rabota-news', 130),\n",
       " ('main-internet-rabota', 129),\n",
       " ('main-archive-news', 126),\n",
       " ('main-rabota-internet', 124),\n",
       " ('main-internet-archive', 123),\n",
       " ('main-archive-bonus', 117),\n",
       " ('main-internet-bonus', 115),\n",
       " ('main-tariffs-internet', 114),\n",
       " ('main-news-archive', 113),\n",
       " ('main-news-internet', 108),\n",
       " ('main-archive-tariffs', 104),\n",
       " ('main-internet-news', 103),\n",
       " ('main-tariffs-archive', 103),\n",
       " ('main-rabota-main', 94)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_tab = spark.sparkContext.textFile(\"hdfs:/data/clickstream.csv\").map(lambda i: i.split('\\t')).persist()\n",
    "\n",
    "fin_tab_0 = rdd_tab.filter(lambda i: 'ERROR' in i[2].upper()).map(lambda i: ((i[0], i[1]), i[4])).reduceByKey(lambda x, y: min(x, y))\n",
    "fin_tab_1 = rdd_tab.filter(lambda i: i[2].upper() in ('PAGE', 'EVENT')).map(lambda i: ((i[0], i[1]), (i[3], i[4]))).leftOuterJoin(fin_tab_0).filter(lambda i: i[1][1] is None or (i[1][0][1] < i[1][1]))\n",
    "fin_tab_2 = fin_tab_1.map(lambda i: ((i[0][0], i[0][1]), (i[1][0][0], i[1][0][1]))).groupByKey().mapValues(lambda j: [page for page, _ in sorted(j, key=lambda y: (y[1], y[0]))])\n",
    "\n",
    "def remove_subsequent_repetitions(route):\n",
    "    return '-'.join([route[i] for i in range(len(route)) if i == 0 or route[i] != route[i - 1]])\n",
    "\n",
    "unique_routes = fin_tab_2.mapValues(remove_subsequent_repetitions)\n",
    "route_counts = unique_routes.map(lambda i: (i[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "spark_rdd_result = spark.sparkContext.parallelize(route_counts.takeOrdered(30, key=lambda x: -x[1])).map(lambda i: (i[0], i[1]))\n",
    "spark_rdd_result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_result = spark_rdd_result.map(lambda i: Row(route=i[0], count=i[1])).toDF()\n",
    "spark_rdd_result.write.mode(\"overwrite\").csv(\"spark_rdd_result.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark DF</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was previously mentioned except for the '%error%' like event types there are only 'page' and 'event' phases, so I have filtered such events \".isin(\"page\", \"event\")\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|route                |count|\n",
      "+---------------------+-----+\n",
      "|main                 |8184 |\n",
      "|main-archive         |1112 |\n",
      "|main-rabota          |1047 |\n",
      "|main-internet        |895  |\n",
      "|main-bonus           |869  |\n",
      "|main-news            |769  |\n",
      "|main-tariffs         |677  |\n",
      "|main-online          |587  |\n",
      "|main-vklad           |518  |\n",
      "|main-rabota-archive  |170  |\n",
      "|main-archive-rabota  |167  |\n",
      "|main-bonus-archive   |143  |\n",
      "|main-rabota-bonus    |138  |\n",
      "|main-bonus-rabota    |135  |\n",
      "|main-news-rabota     |135  |\n",
      "|main-archive-internet|132  |\n",
      "|main-rabota-news     |130  |\n",
      "|main-internet-rabota |129  |\n",
      "|main-archive-news    |126  |\n",
      "|main-rabota-internet |124  |\n",
      "|main-internet-archive|123  |\n",
      "|main-archive-bonus   |117  |\n",
      "|main-internet-bonus  |115  |\n",
      "|main-tariffs-internet|114  |\n",
      "|main-news-archive    |113  |\n",
      "|main-news-internet   |108  |\n",
      "|main-archive-tariffs |104  |\n",
      "|main-internet-news   |103  |\n",
      "|main-tariffs-archive |103  |\n",
      "|main-rabota-main     |94   |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"delimiter\", \"\\t\").csv(\"hdfs:/data/clickstream.csv\", header=True)\n",
    "fin_tab_0 = (df.filter(F.col(\"event_type\").contains(\"error\")).groupBy(\"user_id\", \"session_id\").agg(F.min(\"timestamp\").alias(\"min_error_time\")))\n",
    "fin_tab_1 = (df.alias(\"t0\").join(fin_tab_0.alias(\"t1\"), (F.col(\"t0.user_id\") == F.col(\"t1.user_id\")) & (F.col(\"t0.session_id\") == F.col(\"t1.session_id\")), \"left_outer\")\n",
    ".filter((F.col(\"t0.event_type\").isin(\"page\", \"event\")) & ((F.col(\"t1.min_error_time\").isNull()) | (F.col(\"t0.timestamp\") < F.col(\"t1.min_error_time\"))))\n",
    ".select(\"t0.user_id\", \"t0.session_id\", \"t0.event_page\", \"t0.timestamp\"))\n",
    "\n",
    "def remove_consecutive_repetitions(event_pages):\n",
    "    return [event_pages[i] for i in range(len(event_pages)) if i == 0 or event_pages[i] != event_pages[i - 1]]\n",
    "\n",
    "remove_repetitions_udf = F.udf(remove_consecutive_repetitions, ArrayType(StringType()))\n",
    "fin_tab_2 = (fin_tab_1.withColumn(\"event_pages\", remove_repetitions_udf(F.collect_list(\"event_page\").over(Window.partitionBy(\"user_id\", \"session_id\")\n",
    ".orderBy(\"timestamp\", \"event_page\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))))\n",
    ".select(\"user_id\", \"session_id\", F.concat_ws('-', \"event_pages\").alias(\"route\")).distinct())\n",
    "\n",
    "spark_df_result = (fin_tab_2.groupBy(\"route\").agg(F.count(\"session_id\").alias(\"count\")).orderBy(F.desc(\"count\")).limit(30))\n",
    "spark_df_result.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_result.toPandas().to_csv(\"spark_df_result.csv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comparison between the results obtained:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>compare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(main, main, main)</th>\n",
       "      <td>8184</td>\n",
       "      <td>8184</td>\n",
       "      <td>8184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-archive, main-archive, main-archive)</th>\n",
       "      <td>1112</td>\n",
       "      <td>1112</td>\n",
       "      <td>1112</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-rabota, main-rabota, main-rabota)</th>\n",
       "      <td>1047</td>\n",
       "      <td>1047</td>\n",
       "      <td>1047</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-internet, main-internet, main-internet)</th>\n",
       "      <td>895</td>\n",
       "      <td>895</td>\n",
       "      <td>895</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-bonus, main-bonus, main-bonus)</th>\n",
       "      <td>869</td>\n",
       "      <td>869</td>\n",
       "      <td>869</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-news, main-news, main-news)</th>\n",
       "      <td>769</td>\n",
       "      <td>769</td>\n",
       "      <td>769</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-tariffs, main-tariffs, main-tariffs)</th>\n",
       "      <td>677</td>\n",
       "      <td>677</td>\n",
       "      <td>677</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-online, main-online, main-online)</th>\n",
       "      <td>587</td>\n",
       "      <td>587</td>\n",
       "      <td>587</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-vklad, main-vklad, main-vklad)</th>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-rabota-archive, main-rabota-archive, main-rabota-archive)</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-archive-rabota, main-archive-rabota, main-archive-rabota)</th>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-bonus-archive, main-bonus-archive, main-bonus-archive)</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-rabota-bonus, main-rabota-bonus, main-rabota-bonus)</th>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-news-rabota, main-news-rabota, main-news-rabota)</th>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-bonus-rabota, main-bonus-rabota, main-bonus-rabota)</th>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-archive-internet, main-archive-internet, main-archive-internet)</th>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-rabota-news, main-rabota-news, main-rabota-news)</th>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-internet-rabota, main-internet-rabota, main-internet-rabota)</th>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-archive-news, main-archive-news, main-archive-news)</th>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-rabota-internet, main-rabota-internet, main-rabota-internet)</th>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-internet-archive, main-internet-archive, main-internet-archive)</th>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-archive-bonus, main-archive-bonus, main-archive-bonus)</th>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-internet-bonus, main-internet-bonus, main-internet-bonus)</th>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-tariffs-internet, main-tariffs-internet, main-tariffs-internet)</th>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-news-archive, main-news-archive, main-news-archive)</th>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-news-internet, main-news-internet, main-news-internet)</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-archive-tariffs, main-archive-tariffs, main-archive-tariffs)</th>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-internet-news, main-internet-news, main-internet-news)</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-tariffs-archive, main-tariffs-archive, main-tariffs-archive)</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(main-rabota-main, main-rabota-main, main-rabota-main)</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    count  count  count  \\\n",
       "route                                                                     \n",
       "(main, main, main)                                   8184   8184   8184   \n",
       "(main-archive, main-archive, main-archive)           1112   1112   1112   \n",
       "(main-rabota, main-rabota, main-rabota)              1047   1047   1047   \n",
       "(main-internet, main-internet, main-internet)         895    895    895   \n",
       "(main-bonus, main-bonus, main-bonus)                  869    869    869   \n",
       "(main-news, main-news, main-news)                     769    769    769   \n",
       "(main-tariffs, main-tariffs, main-tariffs)            677    677    677   \n",
       "(main-online, main-online, main-online)               587    587    587   \n",
       "(main-vklad, main-vklad, main-vklad)                  518    518    518   \n",
       "(main-rabota-archive, main-rabota-archive, main...    170    170    170   \n",
       "(main-archive-rabota, main-archive-rabota, main...    167    167    167   \n",
       "(main-bonus-archive, main-bonus-archive, main-b...    143    143    143   \n",
       "(main-rabota-bonus, main-rabota-bonus, main-rab...    138    138    138   \n",
       "(main-news-rabota, main-news-rabota, main-news-...    135    135    135   \n",
       "(main-bonus-rabota, main-bonus-rabota, main-bon...    135    135    135   \n",
       "(main-archive-internet, main-archive-internet, ...    132    132    132   \n",
       "(main-rabota-news, main-rabota-news, main-rabot...    130    130    130   \n",
       "(main-internet-rabota, main-internet-rabota, ma...    129    129    129   \n",
       "(main-archive-news, main-archive-news, main-arc...    126    126    126   \n",
       "(main-rabota-internet, main-rabota-internet, ma...    124    124    124   \n",
       "(main-internet-archive, main-internet-archive, ...    123    123    123   \n",
       "(main-archive-bonus, main-archive-bonus, main-a...    117    117    117   \n",
       "(main-internet-bonus, main-internet-bonus, main...    115    115    115   \n",
       "(main-tariffs-internet, main-tariffs-internet, ...    114    114    114   \n",
       "(main-news-archive, main-news-archive, main-new...    113    113    113   \n",
       "(main-news-internet, main-news-internet, main-n...    108    108    108   \n",
       "(main-archive-tariffs, main-archive-tariffs, ma...    104    104    104   \n",
       "(main-internet-news, main-internet-news, main-i...    103    103    103   \n",
       "(main-tariffs-archive, main-tariffs-archive, ma...    103    103    103   \n",
       "(main-rabota-main, main-rabota-main, main-rabot...     94     94     94   \n",
       "\n",
       "                                                    compare  \n",
       "route                                                        \n",
       "(main, main, main)                                     True  \n",
       "(main-archive, main-archive, main-archive)             True  \n",
       "(main-rabota, main-rabota, main-rabota)                True  \n",
       "(main-internet, main-internet, main-internet)          True  \n",
       "(main-bonus, main-bonus, main-bonus)                   True  \n",
       "(main-news, main-news, main-news)                      True  \n",
       "(main-tariffs, main-tariffs, main-tariffs)             True  \n",
       "(main-online, main-online, main-online)                True  \n",
       "(main-vklad, main-vklad, main-vklad)                   True  \n",
       "(main-rabota-archive, main-rabota-archive, main...     True  \n",
       "(main-archive-rabota, main-archive-rabota, main...     True  \n",
       "(main-bonus-archive, main-bonus-archive, main-b...     True  \n",
       "(main-rabota-bonus, main-rabota-bonus, main-rab...     True  \n",
       "(main-news-rabota, main-news-rabota, main-news-...     True  \n",
       "(main-bonus-rabota, main-bonus-rabota, main-bon...     True  \n",
       "(main-archive-internet, main-archive-internet, ...     True  \n",
       "(main-rabota-news, main-rabota-news, main-rabot...     True  \n",
       "(main-internet-rabota, main-internet-rabota, ma...     True  \n",
       "(main-archive-news, main-archive-news, main-arc...     True  \n",
       "(main-rabota-internet, main-rabota-internet, ma...     True  \n",
       "(main-internet-archive, main-internet-archive, ...     True  \n",
       "(main-archive-bonus, main-archive-bonus, main-a...     True  \n",
       "(main-internet-bonus, main-internet-bonus, main...     True  \n",
       "(main-tariffs-internet, main-tariffs-internet, ...     True  \n",
       "(main-news-archive, main-news-archive, main-new...     True  \n",
       "(main-news-internet, main-news-internet, main-n...     True  \n",
       "(main-archive-tariffs, main-archive-tariffs, ma...     True  \n",
       "(main-internet-news, main-internet-news, main-i...     True  \n",
       "(main-tariffs-archive, main-tariffs-archive, ma...     True  \n",
       "(main-rabota-main, main-rabota-main, main-rabot...     True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_all = pd.concat([df.toPandas() for df in [spark_sql_result, spark_rdd_result, spark_df_result]], axis=1).set_index('route')\n",
    "compare_all['compare'] = (compare_all.iloc[:, -3:].nunique(axis=1) == 1)\n",
    "compare_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As could be seen the resulted counts are the same for all 3 methods and the logic is consistent between the spark SQL, RDD, and DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
